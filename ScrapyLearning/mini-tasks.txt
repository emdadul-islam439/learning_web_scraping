## Tutorial: 9->
    * get Title of the QuoteTutorial page with CSS (Done)
    * get all the Quotes from the QuoteTutorial page (Done)
    * get individual quotes by their index (Done)
    * get class-name with SelectorGadget and get all the elements related to the class name (Done)

## Tutorial: 10-> 
    * get Title of the QuoteTutorial page with XPATH (Done)
    * get all the Quotes from the QuoteTutorial page (Done)
    * get individual quotes by their index (Done)
    * get class-name with SelectorGadget and get all the elements related to the class name (Done)
    * get 'href' value (link) text [from 'go to next' button] by CSS and XPATH combination (Done)

## Tutorial: 11-> 
    * get first "title", "author", "quote", and "tags"
    * get all "title", "author", "quote", and "tags"

## Tutorial: 12-> 
    * save the scraped info into temporary container called 'item' [in items.py file]
        - declare the variable, in this case-> 'title', 'author', and 'tags'
        - import the 'Item Container Class' into the "quote_spider.py" file. in this case-> "from ..items import QuoteTutorialItem"
        - create a variable. generaly it's called 'item'. in this case "item = QuoteTutorialItem()"
        - assign the respective values to the variables. in this case-> "item.title = title"
        - "yield" the item. in this case "yield item"

## Tutorial: 13-> 
    * save scraped info into xml/json/csv file
        - in this case-> in the command line, use the command 'scrapy crawl quotes -o items.xml', an xml file will be generated automatically

## Tutorial: 14->
    * setting up the "pipeline"
        - in the settings.py file, un-comment the "ITEM_PIPELINES" variable, and the pipeline will be activated
        - in the pipeline.py file, you can print different variable's value. in this case-> print(item['title'][0]) inside the "process_item" function
        - if executed successfully, you can see the printed value in the terminal   

## Tutorial: 15->
    * shown how to create a SqLite table and how to manually insert values into it 
        - create a temporary file "database.py"
        - import sqlite3 
        - create some variables:
            -> connect = sqlite3.connect('<db-name>.db'),
            -> cursor = connect.cursor()
            -> cursor.execute("""create table quotes_table(, , ,)""")
            -> connect.commit()
            -> connect.close()

## Tutorial: 16->
    * creating 3 functions-> "create_connection", "create_database" and "store_db"
    * calling 3 function from "__init__" and "process_item" function
    * crawl the spider and see if successfully storing items into db

## Tutorial: 17->
    * 